{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IBKR Analytics Database Analysis\n",
        "\n",
        "This notebook connects to the same SQLite database used by the backend to enable interactive analysis of trading data.\n",
        "\n",
        "## Database Overview\n",
        "\n",
        "The database contains the following tables:\n",
        "- **pnl_history**: Historical PnL records with net liquidation values\n",
        "- **account_snapshots**: Account snapshots at different timestamps\n",
        "- **positions**: Current position snapshots\n",
        "- **trades**: Trade execution records\n",
        "- **performance_metrics**: Calculated performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/zelin/Desktop/PA Investment/Invest_strategy\n",
            "Python path includes: False\n"
          ]
        }
      ],
      "source": [
        "# Setup: Add project root to Python path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Get the project root directory (parent of notebooks directory)\n",
        "# In Jupyter notebooks, Path().resolve() gives the current working directory\n",
        "# We need to go up one level from the notebooks directory\n",
        "project_root = Path().resolve()\n",
        "# If we're in the notebooks directory, go up one level\n",
        "if project_root.name == 'notebooks':\n",
        "    project_root = project_root.parent\n",
        "\n",
        "project_root_str = str(project_root.resolve())\n",
        "\n",
        "if project_root_str not in sys.path:\n",
        "    sys.path.insert(0, project_root_str)\n",
        "\n",
        "# Change working directory to project root for consistent path resolution\n",
        "os.chdir(project_root_str)\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "print(f\"Python path includes project root: {project_root_str in sys.path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n",
            "Database URL: sqlite:///./ibkr_analytics.db\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from sqlalchemy import create_engine, inspect, func, desc\n",
        "from sqlalchemy.orm import sessionmaker, Session\n",
        "\n",
        "# Import backend modules\n",
        "from backend.config import settings\n",
        "from backend.models import (\n",
        "    PnLHistory, AccountSnapshot, Position, Trade, PerformanceMetric, Base\n",
        ")\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "print(f\"Database URL: {settings.database.url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Database Connection\n",
        "\n",
        "Connect to the same database used by the backend using the shared configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resolved database path: /Users/zelin/Desktop/PA Investment/Invest_strategy/ibkr_analytics.db\n",
            "✓ Database connection successful!\n",
            "✓ Database URL: sqlite:////Users/zelin/Desktop/PA Investment/Invest_strategy/ibkr_analytics.db\n",
            "\n",
            "✓ Available tables: account_snapshots, performance_metrics, pnl_history, positions, trades\n"
          ]
        }
      ],
      "source": [
        "# Create database engine using the same configuration as backend\n",
        "# For SQLite, ensure relative paths are resolved from project root\n",
        "db_url = settings.database.url\n",
        "if db_url.startswith(\"sqlite\"):\n",
        "    # Convert relative SQLite paths to absolute paths based on project root\n",
        "    if \"./\" in db_url or db_url.startswith(\"sqlite:///\"):\n",
        "        # Extract the database file path\n",
        "        if db_url.startswith(\"sqlite:///./\"):\n",
        "            db_file = db_url.replace(\"sqlite:///./\", \"\")\n",
        "        elif db_url.startswith(\"sqlite:///\"):\n",
        "            db_file = db_url.replace(\"sqlite:///\", \"\")\n",
        "        else:\n",
        "            db_file = db_url.split(\"///\")[-1] if \"///\" in db_url else db_url.split(\"://\")[-1]\n",
        "        \n",
        "        # Resolve to absolute path from project root\n",
        "        db_path = (project_root / db_file).resolve()\n",
        "        db_url = f\"sqlite:///{db_path}\"\n",
        "        print(f\"Resolved database path: {db_path}\")\n",
        "    \n",
        "    engine = create_engine(\n",
        "        db_url,\n",
        "        connect_args={\"check_same_thread\": False},\n",
        "        echo=settings.database.echo,\n",
        "    )\n",
        "else:\n",
        "    engine = create_engine(\n",
        "        db_url,\n",
        "        echo=settings.database.echo,\n",
        "    )\n",
        "\n",
        "# Create session factory\n",
        "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
        "\n",
        "# Test connection\n",
        "with engine.connect() as conn:\n",
        "    print(\"✓ Database connection successful!\")\n",
        "    print(f\"✓ Database URL: {db_url}\")\n",
        "    \n",
        "    # List all tables\n",
        "    inspector = inspect(engine)\n",
        "    tables = inspector.get_table_names()\n",
        "    print(f\"\\n✓ Available tables: {', '.join(tables)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "Utility functions for common query patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_to_df(query_result, model_class=None):\n",
        "    \"\"\"Convert SQLAlchemy query results to pandas DataFrame.\"\"\"\n",
        "    if isinstance(query_result, list):\n",
        "        if len(query_result) == 0:\n",
        "            return pd.DataFrame()\n",
        "        # Convert list of model instances to dicts\n",
        "        data = []\n",
        "        for row in query_result:\n",
        "            if hasattr(row, '__dict__'):\n",
        "                row_dict = {k: v for k, v in row.__dict__.items() if not k.startswith('_')}\n",
        "                data.append(row_dict)\n",
        "            else:\n",
        "                data.append(row)\n",
        "        return pd.DataFrame(data)\n",
        "    else:\n",
        "        # Single result\n",
        "        if hasattr(query_result, '__dict__'):\n",
        "            return pd.DataFrame([{k: v for k, v in query_result.__dict__.items() if not k.startswith('_')}])\n",
        "        return pd.DataFrame([query_result])\n",
        "\n",
        "def get_session():\n",
        "    \"\"\"Get a database session.\"\"\"\n",
        "    return SessionLocal()\n",
        "\n",
        "print(\"Helper functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PnL History Analysis\n",
        "\n",
        "Query the `pnl_history` table which contains net liquidation values and PnL breakdowns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all PnL history records\n",
        "with get_session() as db:\n",
        "    pnl_records = db.query(PnLHistory).order_by(PnLHistory.date).all()\n",
        "    pnl_df = query_to_df(pnl_records)\n",
        "    \n",
        "print(f\"Total PnL records: {len(pnl_df)}\")\n",
        "if not pnl_df.empty:\n",
        "    print(f\"\\nDate range: {pnl_df['date'].min()} to {pnl_df['date'].max()}\")\n",
        "    print(f\"\\nFirst few records:\")\n",
        "    display(pnl_df.head())\n",
        "    print(f\"\\nDataFrame info:\")\n",
        "    pnl_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get latest net liquidation value\n",
        "with get_session() as db:\n",
        "    latest_pnl = db.query(PnLHistory).order_by(desc(PnLHistory.date)).first()\n",
        "    \n",
        "if latest_pnl:\n",
        "    print(f\"Latest Net Liquidation: ${latest_pnl.net_liquidation:,.2f}\")\n",
        "    print(f\"Date: {latest_pnl.date}\")\n",
        "    print(f\"Total PnL: ${latest_pnl.total_pnl:,.2f}\")\n",
        "    print(f\"Realized PnL: ${latest_pnl.realized_pnl:,.2f}\")\n",
        "    print(f\"Unrealized PnL: ${latest_pnl.unrealized_pnl:,.2f}\")\n",
        "else:\n",
        "    print(\"No PnL records found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter PnL by date range\n",
        "start_date = datetime.now() - timedelta(days=30)  # Last 30 days\n",
        "end_date = datetime.now()\n",
        "\n",
        "with get_session() as db:\n",
        "    recent_pnl = db.query(PnLHistory).filter(\n",
        "        PnLHistory.date >= start_date,\n",
        "        PnLHistory.date <= end_date\n",
        "    ).order_by(PnLHistory.date).all()\n",
        "    \n",
        "    recent_pnl_df = query_to_df(recent_pnl)\n",
        "\n",
        "if not recent_pnl_df.empty:\n",
        "    print(f\"Records in last 30 days: {len(recent_pnl_df)}\")\n",
        "    display(recent_pnl_df)\n",
        "else:\n",
        "    print(\"No records found in the specified date range\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate daily returns from net liquidation\n",
        "if not pnl_df.empty and 'net_liquidation' in pnl_df.columns:\n",
        "    pnl_df['date'] = pd.to_datetime(pnl_df['date'])\n",
        "    pnl_df = pnl_df.sort_values('date')\n",
        "    pnl_df['daily_return'] = pnl_df['net_liquidation'].pct_change()\n",
        "    pnl_df['cumulative_return'] = (1 + pnl_df['daily_return']).cumprod() - 1\n",
        "    \n",
        "    print(\"Daily Returns Analysis:\")\n",
        "    print(f\"Average daily return: {pnl_df['daily_return'].mean():.4%}\")\n",
        "    print(f\"Std dev of daily returns: {pnl_df['daily_return'].std():.4%}\")\n",
        "    print(f\"\\nRecent returns:\")\n",
        "    display(pnl_df[['date', 'net_liquidation', 'daily_return', 'cumulative_return']].tail(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Account Snapshots\n",
        "\n",
        "Query account snapshots to see account values over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all account snapshots\n",
        "with get_session() as db:\n",
        "    snapshots = db.query(AccountSnapshot).order_by(AccountSnapshot.timestamp).all()\n",
        "    snapshots_df = query_to_df(snapshots)\n",
        "    \n",
        "print(f\"Total account snapshots: {len(snapshots_df)}\")\n",
        "if not snapshots_df.empty:\n",
        "    print(f\"\\nDate range: {snapshots_df['timestamp'].min()} to {snapshots_df['timestamp'].max()}\")\n",
        "    print(f\"\\nLatest snapshot:\")\n",
        "    display(snapshots_df.tail(1))\n",
        "    print(f\"\\nAll columns:\")\n",
        "    print(snapshots_df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get latest account snapshot\n",
        "with get_session() as db:\n",
        "    latest_snapshot = db.query(AccountSnapshot).order_by(desc(AccountSnapshot.timestamp)).first()\n",
        "    \n",
        "if latest_snapshot:\n",
        "    print(\"Latest Account Snapshot:\")\n",
        "    print(f\"Account ID: {latest_snapshot.account_id}\")\n",
        "    print(f\"Timestamp: {latest_snapshot.timestamp}\")\n",
        "    net_liq = latest_snapshot.net_liquidation\n",
        "    print(f\"Net Liquidation: ${net_liq:,.2f}\" if net_liq is not None else \"Net Liquidation: N/A\")\n",
        "    total_cash = latest_snapshot.total_cash_value\n",
        "    print(f\"Total Cash Value: ${total_cash:,.2f}\" if total_cash is not None else \"Total Cash Value: N/A\")\n",
        "    buying_power = latest_snapshot.buying_power\n",
        "    print(f\"Buying Power: ${buying_power:,.2f}\" if buying_power is not None else \"Buying Power: N/A\")\n",
        "    equity = latest_snapshot.equity\n",
        "    print(f\"Equity: ${equity:,.2f}\" if equity is not None else \"Equity: N/A\")\n",
        "else:\n",
        "    print(\"No account snapshots found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positions Analysis\n",
        "\n",
        "Analyze current and historical positions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all positions\n",
        "with get_session() as db:\n",
        "    positions = db.query(Position).order_by(desc(Position.timestamp)).all()\n",
        "    positions_df = query_to_df(positions)\n",
        "    \n",
        "print(f\"Total position records: {len(positions_df)}\")\n",
        "if not positions_df.empty:\n",
        "    print(f\"\\nUnique symbols: {positions_df['symbol'].nunique()}\")\n",
        "    print(f\"\\nLatest positions:\")\n",
        "    display(positions_df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get latest position for each symbol\n",
        "if not positions_df.empty:\n",
        "    positions_df['timestamp'] = pd.to_datetime(positions_df['timestamp'])\n",
        "    latest_positions = positions_df.sort_values('timestamp').groupby('symbol').tail(1)\n",
        "    \n",
        "    print(\"Latest Position for Each Symbol:\")\n",
        "    display(latest_positions[['symbol', 'quantity', 'avg_cost', 'market_price', 'market_value', 'unrealized_pnl', 'timestamp']])\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(f\"\\nTotal Market Value: ${latest_positions['market_value'].sum():,.2f}\")\n",
        "    print(f\"Total Unrealized PnL: ${latest_positions['unrealized_pnl'].sum():,.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trades Analysis\n",
        "\n",
        "Query trade execution history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all trades\n",
        "with get_session() as db:\n",
        "    trades = db.query(Trade).order_by(desc(Trade.exec_time)).all()\n",
        "    trades_df = query_to_df(trades)\n",
        "    \n",
        "print(f\"Total trades: {len(trades_df)}\")\n",
        "if not trades_df.empty:\n",
        "    print(f\"\\nDate range: {trades_df['exec_time'].min()} to {trades_df['exec_time'].max()}\")\n",
        "    print(f\"\\nRecent trades:\")\n",
        "    display(trades_df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trade volume analysis by symbol\n",
        "if not trades_df.empty:\n",
        "    trades_df['exec_time'] = pd.to_datetime(trades_df['exec_time'])\n",
        "    \n",
        "    # Group by symbol\n",
        "    trade_summary = trades_df.groupby('symbol').agg({\n",
        "        'shares': ['sum', 'count'],\n",
        "        'price': 'mean',\n",
        "        'commission': 'sum'\n",
        "    }).round(2)\n",
        "    \n",
        "    trade_summary.columns = ['Total_Shares', 'Trade_Count', 'Avg_Price', 'Total_Commission']\n",
        "    trade_summary = trade_summary.sort_values('Total_Shares', ascending=False)\n",
        "    \n",
        "    print(\"Trade Summary by Symbol:\")\n",
        "    display(trade_summary)\n",
        "    # Trade count by side\n",
        "    print(\"\\nTrade Count by Side:\")\n",
        "    display(trades_df['side'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Metrics\n",
        "\n",
        "Analyze calculated performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all performance metrics\n",
        "with get_session() as db:\n",
        "    metrics = db.query(PerformanceMetric).order_by(desc(PerformanceMetric.date)).all()\n",
        "    metrics_df = query_to_df(metrics)\n",
        "    \n",
        "print(f\"Total performance metric records: {len(metrics_df)}\")\n",
        "if not metrics_df.empty:\n",
        "    print(f\"\\nDate range: {metrics_df['date'].min()} to {metrics_df['date'].max()}\")\n",
        "    print(f\"\\nLatest metrics:\")\n",
        "    display(metrics_df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get latest performance metrics\n",
        "if not metrics_df.empty:\n",
        "    latest_metrics = metrics_df.iloc[0]  # Already sorted by date desc\n",
        "    \n",
        "    print(\"Latest Performance Metrics:\")\n",
        "    print(f\"Date: {latest_metrics['date']}\")\n",
        "    print(f\"Daily Return: {latest_metrics['daily_return']:.4%}\" if pd.notna(latest_metrics['daily_return']) else \"Daily Return: N/A\")\n",
        "    print(f\"Cumulative Return: {latest_metrics['cumulative_return']:.4%}\" if pd.notna(latest_metrics['cumulative_return']) else \"Cumulative Return: N/A\")\n",
        "    print(f\"Sharpe Ratio: {latest_metrics['sharpe_ratio']:.2f}\" if pd.notna(latest_metrics['sharpe_ratio']) else \"Sharpe Ratio: N/A\")\n",
        "    print(f\"Sortino Ratio: {latest_metrics['sortino_ratio']:.2f}\" if pd.notna(latest_metrics['sortino_ratio']) else \"Sortino Ratio: N/A\")\n",
        "    print(f\"Max Drawdown: {latest_metrics['max_drawdown']:.4%}\" if pd.notna(latest_metrics['max_drawdown']) else \"Max Drawdown: N/A\")\n",
        "    print(f\"Win Rate: {latest_metrics['win_rate']:.2%}\" if pd.notna(latest_metrics['win_rate']) else \"Win Rate: N/A\")\n",
        "    print(f\"Total Trades: {latest_metrics['total_trades']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Analysis\n",
        "\n",
        "Use the cells below to write your own custom queries and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Query specific account_id\n",
        "# account_id = \"YOUR_ACCOUNT_ID\"\n",
        "# with get_session() as db:\n",
        "#     account_pnl = db.query(PnLHistory).filter(\n",
        "#         PnLHistory.account_id == account_id\n",
        "#     ).order_by(PnLHistory.date).all()\n",
        "#     account_pnl_df = query_to_df(account_pnl)\n",
        "#     display(account_pnl_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Raw SQL query using pandas\n",
        "# query = \"SELECT * FROM pnl_history WHERE date >= '2024-01-01' ORDER BY date\"\n",
        "# df = pd.read_sql_query(query, engine)\n",
        "# display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Aggregate queries\n",
        "# with get_session() as db:\n",
        "#     result = db.query(\n",
        "#         func.count(PnLHistory.id).label('count'),\n",
        "#         func.avg(PnLHistory.net_liquidation).label('avg_net_liq'),\n",
        "#         func.max(PnLHistory.net_liquidation).label('max_net_liq'),\n",
        "#         func.min(PnLHistory.net_liquidation).label('min_net_liq')\n",
        "#     ).first()\n",
        "#     print(f\"Count: {result.count}\")\n",
        "#     print(f\"Avg Net Liquidation: ${result.avg_net_liq:,.2f}\")\n",
        "#     print(f\"Max Net Liquidation: ${result.max_net_liq:,.2f}\")\n",
        "#     print(f\"Min Net Liquidation: ${result.min_net_liq:,.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization Examples\n",
        "\n",
        "Example code for creating visualizations (requires matplotlib or plotly)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Plot net liquidation over time\n",
        "# import matplotlib.pyplot as plt\n",
        "# \n",
        "# if not pnl_df.empty and 'net_liquidation' in pnl_df.columns:\n",
        "#     pnl_df['date'] = pd.to_datetime(pnl_df['date'])\n",
        "#     pnl_df = pnl_df.sort_values('date')\n",
        "#     \n",
        "#     plt.figure(figsize=(12, 6))\n",
        "#     plt.plot(pnl_df['date'], pnl_df['net_liquidation'])\n",
        "#     plt.title('Net Liquidation Over Time')\n",
        "#     plt.xlabel('Date')\n",
        "#     plt.ylabel('Net Liquidation ($)')\n",
        "#     plt.grid(True)\n",
        "#     plt.xticks(rotation=45)\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Plot PnL breakdown\n",
        "# import plotly.graph_objects as go\n",
        "# \n",
        "# if not pnl_df.empty:\n",
        "#     fig = go.Figure()\n",
        "#     fig.add_trace(go.Scatter(x=pnl_df['date'], y=pnl_df['realized_pnl'], \n",
        "#                              name='Realized PnL', mode='lines'))\n",
        "#     fig.add_trace(go.Scatter(x=pnl_df['date'], y=pnl_df['unrealized_pnl'], \n",
        "#                              name='Unrealized PnL', mode='lines'))\n",
        "#     fig.add_trace(go.Scatter(x=pnl_df['date'], y=pnl_df['total_pnl'], \n",
        "#                              name='Total PnL', mode='lines'))\n",
        "#     fig.update_layout(title='PnL Breakdown Over Time', \n",
        "#                       xaxis_title='Date', \n",
        "#                       yaxis_title='PnL ($)')\n",
        "#     fig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ibkr-analytics",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
